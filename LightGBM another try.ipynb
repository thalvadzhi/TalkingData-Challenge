{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devthebear_gmail_com/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data...\n",
      "loading test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devthebear_gmail_com/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.1,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 10,\n",
    "        'verbose': 0,\n",
    "        'metric':metrics\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "\n",
    "    n_estimators = bst1.best_iteration\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][n_estimators-1])\n",
    "\n",
    "    return bst1\n",
    "\n",
    "#path = '../input/'\n",
    "path = './'\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "print('loading train data...')\n",
    "#train_df = pd.read_csv(path+\"train_sample.csv\", skiprows=range(1,144903891), nrows=40000000, dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "train_df = pd.read_csv(path+\"train.csv\", skiprows=range(1,1), dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "\n",
    "print('loading test data...')\n",
    "test_df = pd.read_csv(path+\"test.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "len_train = len(train_df)\n",
    "train_df=train_df.append(test_df)\n",
    "\n",
    "del test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting new features...\n",
      "grouping by ip-day-hour combination...\n",
      "grouping by ip-app combination...\n",
      "grouping by : ip_app_day_hour\n",
      "grouping by : ip_app_day_h_ch_var_hour\n",
      "grouping by : ip_app_day_h_ch_var_day\n",
      "grouping by ip-day-hour combination...\n",
      "grouping by ip-app combination...\n",
      "grouping by ip-app-os combination...\n",
      "grouping by : ip_day_chl_var_hour\n",
      "grouping by : ip_app_os_var_hour\n",
      "grouping by : ip_app_channel_var_day\n",
      "grouping by : ip_app_chl_mean_hour\n",
      "merging...\n",
      "vars and data type: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 203694359 entries, 0 to 203694358\n",
      "Data columns (total 22 columns):\n",
      "app                         uint16\n",
      "channel                     uint16\n",
      "click_id                    float64\n",
      "click_time                  object\n",
      "device                      uint16\n",
      "ip                          uint32\n",
      "is_attributed               float64\n",
      "os                          uint16\n",
      "hour                        uint8\n",
      "day                         uint8\n",
      "ip_day_hour_count           int64\n",
      "ip_app_to_os_count          int64\n",
      "ip_app_day_hour_count       int64\n",
      "iacd_var                    float64\n",
      "iach_var                    float64\n",
      "ip_tcount                   int64\n",
      "ip_app_count                int64\n",
      "ip_app_os_count             int64\n",
      "ip_tchan_count              float64\n",
      "ip_app_os_var               float64\n",
      "ip_app_channel_var_day      float64\n",
      "ip_app_channel_mean_hour    float64\n",
      "dtypes: float64(8), int64(6), object(1), uint16(4), uint32(1), uint8(2)\n",
      "memory usage: 26.9+ GB\n",
      "len train .................................... 184903890\n",
      "train size:  183103890\n",
      "valid size:  1800000\n",
      "test size :  18790469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Extracting new features...')\n",
    "train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "###\n",
    "'''' \n",
    "print('grouping by ip-app-channel combination...')\n",
    "gp = train_df[['ip', 'day', 'channel', 'hour']].groupby(by=['ip', 'day', 'channel'])[['hour']].count().reset_index().rename(index=str, columns={'hour': 'ip_day_channel_count'})\n",
    "train_df - train_df.merge(gp, on=['ip', 'day', 'channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "'''\n",
    "\n",
    "print('grouping by ip-day-hour combination...')\n",
    "gp = train_df[['ip', 'day', 'hour', 'channel']].groupby(by=['ip', 'day', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_day_hour_count'})\n",
    "train_df = train_df.merge(gp, on=['ip', 'day', 'hour'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by ip-app combination...')\n",
    "gp = train_df[['ip','app', 'os']].groupby(by=['ip', 'app'])[['os']].count().reset_index().rename(index=str, columns={'os': 'ip_app_to_os_count'})\n",
    "train_df = train_df.merge(gp, on=['ip', 'app'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_day_hour')\n",
    "gp = train_df[['ip', 'app', 'day', 'hour', 'channel']].groupby(by=['ip', 'app', 'day', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_day_hour_count'})\n",
    "train_df = train_df.merge(gp, on=['ip', 'app', 'day', 'hour'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_day_h_ch_var_hour')\n",
    "gp = train_df[['ip', 'app', 'day', 'hour', 'channel']].groupby(by=['ip','app','channel', 'day'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'iacd_var'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'day','channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_day_h_ch_var_day')\n",
    "gp = train_df[['ip', 'app', 'day', 'hour', 'channel']].groupby(by=['ip','app','channel', 'hour'])[['day']].var().reset_index().rename(index=str, columns={'day': 'iach_var'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'hour','channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "print('grouping by ip-day-hour combination...')\n",
    "gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_tcount'})\n",
    "train_df = train_df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by ip-app combination...')\n",
    "gp = train_df[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','app'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print('grouping by ip-app-os combination...')\n",
    "gp = train_df[['ip','app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_os_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Adding features with var and mean hour (inspired from nuhsikander's script)\n",
    "print('grouping by : ip_day_chl_var_hour')\n",
    "gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','channel'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_tchan_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','day','channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_os_var_hour')\n",
    "gp = train_df[['ip','app', 'os', 'hour']].groupby(by=['ip', 'app', 'os'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_app_os_var'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_channel_var_day')\n",
    "gp = train_df[['ip','app', 'channel', 'day']].groupby(by=['ip', 'app', 'channel'])[['day']].var().reset_index().rename(index=str, columns={'day': 'ip_app_channel_var_day'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_chl_mean_hour')\n",
    "gp = train_df[['ip','app', 'channel','hour']].groupby(by=['ip', 'app', 'channel'])[['hour']].mean().reset_index().rename(index=str, columns={'hour': 'ip_app_channel_mean_hour'})\n",
    "print(\"merging...\")\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print(\"vars and data type: \")\n",
    "train_df.info()\n",
    "train_df['ip_tcount'] = train_df['ip_tcount'].astype('uint16')\n",
    "train_df['ip_app_count'] = train_df['ip_app_count'].astype('uint16')\n",
    "train_df['ip_app_os_count'] = train_df['ip_app_os_count'].astype('uint16')\n",
    "train_df['ip_day_hour_count'] = train_df['ip_day_hour_count'].astype('uint16')\n",
    "train_df['ip_app_to_os_count'] = train_df['ip_app_to_os_count'].astype('uint16')\n",
    "train_df['ip_app_day_hour_count'] = train_df['ip_app_day_hour_count'].astype('uint16')\n",
    "\n",
    "print(\"len train ....................................\", len_train)\n",
    "test_df = train_df[len_train:]\n",
    "#val_df = train_df[(len_train-2500000):len_train]\n",
    "val_df = train_df[(len_train-1800000):len_train]\n",
    "train_df = train_df[:(len_train-1800000)]\n",
    "\n",
    "print(\"train size: \", len(train_df))\n",
    "print(\"valid size: \", len(val_df))\n",
    "print(\"test size : \", len(test_df))\n",
    "\n",
    "target = 'is_attributed'\n",
    "predictors = ['app','device','os', 'channel', 'hour', 'day', \n",
    "              'ip_tcount', 'ip_tchan_count', 'ip_app_count',\n",
    "              'ip_app_os_count', 'ip_app_os_var',\n",
    "              'ip_app_channel_var_day', 'ip_app_channel_mean_hour',\n",
    "              'ip_day_hour_count', 'ip_app_os_count', 'ip_app_day_hour_count',\n",
    "              'iacd_var', 'iach_var']\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_df, open(\"iliyan_train.pickle\", \"wb\"), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_df, open(\"iliyan_test.pickle\", \"wb\"), protocol=4)\n",
    "pickle.dump(val_df, open(\"iliyan_val.pickle\", \"wb\"), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "preparing validation datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devthebear_gmail_com/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1040: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/devthebear_gmail_com/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:685: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[10]\ttrain's auc: 0.962188\tvalid's auc: 0.976449\n",
      "[20]\ttrain's auc: 0.964666\tvalid's auc: 0.978384\n",
      "[30]\ttrain's auc: 0.965922\tvalid's auc: 0.978969\n",
      "[40]\ttrain's auc: 0.96665\tvalid's auc: 0.979573\n",
      "[50]\ttrain's auc: 0.967352\tvalid's auc: 0.980354\n",
      "[60]\ttrain's auc: 0.967804\tvalid's auc: 0.980724\n",
      "[70]\ttrain's auc: 0.968114\tvalid's auc: 0.980995\n",
      "[80]\ttrain's auc: 0.968427\tvalid's auc: 0.981254\n",
      "[90]\ttrain's auc: 0.968719\tvalid's auc: 0.981684\n",
      "[100]\ttrain's auc: 0.968902\tvalid's auc: 0.981891\n",
      "[110]\ttrain's auc: 0.969081\tvalid's auc: 0.982147\n",
      "[120]\ttrain's auc: 0.96925\tvalid's auc: 0.98244\n",
      "[130]\ttrain's auc: 0.969404\tvalid's auc: 0.982748\n",
      "[140]\ttrain's auc: 0.969558\tvalid's auc: 0.982966\n",
      "[150]\ttrain's auc: 0.969683\tvalid's auc: 0.983138\n",
      "[160]\ttrain's auc: 0.96981\tvalid's auc: 0.983254\n",
      "[170]\ttrain's auc: 0.969912\tvalid's auc: 0.983394\n",
      "[180]\ttrain's auc: 0.970013\tvalid's auc: 0.983529\n",
      "[190]\ttrain's auc: 0.97014\tvalid's auc: 0.983677\n",
      "[200]\ttrain's auc: 0.970271\tvalid's auc: 0.98381\n",
      "[210]\ttrain's auc: 0.970358\tvalid's auc: 0.983906\n",
      "[220]\ttrain's auc: 0.970457\tvalid's auc: 0.983994\n",
      "[230]\ttrain's auc: 0.970557\tvalid's auc: 0.984088\n",
      "[240]\ttrain's auc: 0.970647\tvalid's auc: 0.984154\n",
      "[250]\ttrain's auc: 0.970742\tvalid's auc: 0.984254\n",
      "[260]\ttrain's auc: 0.970827\tvalid's auc: 0.984309\n",
      "[270]\ttrain's auc: 0.970899\tvalid's auc: 0.984367\n",
      "[280]\ttrain's auc: 0.970971\tvalid's auc: 0.984479\n",
      "[290]\ttrain's auc: 0.971031\tvalid's auc: 0.984537\n",
      "[300]\ttrain's auc: 0.971084\tvalid's auc: 0.984583\n",
      "[310]\ttrain's auc: 0.971148\tvalid's auc: 0.984638\n",
      "[320]\ttrain's auc: 0.971205\tvalid's auc: 0.984675\n",
      "[330]\ttrain's auc: 0.971261\tvalid's auc: 0.984706\n",
      "[340]\ttrain's auc: 0.97132\tvalid's auc: 0.984754\n",
      "[350]\ttrain's auc: 0.97137\tvalid's auc: 0.984794\n",
      "[360]\ttrain's auc: 0.971425\tvalid's auc: 0.98484\n",
      "[370]\ttrain's auc: 0.971471\tvalid's auc: 0.984874\n",
      "[380]\ttrain's auc: 0.97153\tvalid's auc: 0.984942\n",
      "[390]\ttrain's auc: 0.971606\tvalid's auc: 0.984976\n",
      "[400]\ttrain's auc: 0.971654\tvalid's auc: 0.985016\n",
      "[410]\ttrain's auc: 0.971706\tvalid's auc: 0.98506\n",
      "[420]\ttrain's auc: 0.971762\tvalid's auc: 0.985138\n",
      "[430]\ttrain's auc: 0.971824\tvalid's auc: 0.985157\n",
      "[440]\ttrain's auc: 0.971877\tvalid's auc: 0.985232\n",
      "[450]\ttrain's auc: 0.971926\tvalid's auc: 0.985289\n",
      "[460]\ttrain's auc: 0.971985\tvalid's auc: 0.985375\n",
      "[470]\ttrain's auc: 0.972041\tvalid's auc: 0.985447\n",
      "[480]\ttrain's auc: 0.97209\tvalid's auc: 0.985494\n",
      "[490]\ttrain's auc: 0.972133\tvalid's auc: 0.985555\n",
      "[500]\ttrain's auc: 0.972172\tvalid's auc: 0.985602\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttrain's auc: 0.972172\tvalid's auc: 0.985602\n",
      "\n",
      "Model Report\n",
      "n_estimators :  500\n",
      "auc: 0.9856017859543426\n",
      "[18735.31806588173]: model training time\n",
      "Predicting...\n",
      "writing...\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "    'num_leaves': 7,  # 2^max_depth - 1\n",
    "    'max_depth': 5,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'scale_pos_weight':99 # because training data is extremely unbalanced \n",
    "}\n",
    "bst = lgb_modelfit_nocv(params, \n",
    "                        train_df, \n",
    "                        val_df, \n",
    "                        predictors, \n",
    "                        target, \n",
    "                        objective='binary', \n",
    "                        metrics='auc',\n",
    "                        early_stopping_rounds=30, \n",
    "                        verbose_eval=True, \n",
    "                        num_boost_round=500, \n",
    "                        categorical_features=categorical)\n",
    "\n",
    "print('[{}]: model training time'.format(time.time() - start_time))\n",
    "del train_df\n",
    "del val_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"Predicting...\")\n",
    "sub['is_attributed'] = bst.predict(test_df[predictors])\n",
    "print(\"writing...\")\n",
    "sub.to_csv('ssubmission_iliyan.csv',index=False)\n",
    "print(\"done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.save_model(\"model_iliyan.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path+\"train.csv\", skiprows=range(1,1), dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277396,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"ip\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pickle.load(open(\"iliyan_train.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_lgb = sorted(zip(tr.columns, bst.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True)\n",
    "importance_lgb = pd.DataFrame({'feature': importance_lgb})\n",
    "importance_lgb = importance_lgb.apply(lambda x: pd.Series(x['feature']), axis=1)\n",
    "importance_lgb.columns = ['feature', 'importance']\n",
    "importance_lgb.to_csv('importance-lgb-iliyan.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
