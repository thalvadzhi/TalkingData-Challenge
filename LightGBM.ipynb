{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devthebear_gmail_com/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None, learning_rates=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.01,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 12,\n",
    "        'verbose': 0,\n",
    "        'metric':metrics\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     learning_rates=learning_rates,\n",
    "                     feval=feval)\n",
    "\n",
    "    n_estimators = bst1.best_iteration\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][n_estimators-1])\n",
    "\n",
    "    return bst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data...\n",
      "loading test data...\n"
     ]
    }
   ],
   "source": [
    "path = './'\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "print('loading train data...')\n",
    "train_df = pd.read_csv(path+\"train.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "\n",
    "print('loading test data...')\n",
    "test_df = pd.read_csv(path+\"test.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184903890"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devthebear_gmail_com/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "len_train = len(train_df)\n",
    "train_df=train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting new features...\n",
      "grouping by ip-day-hour combination...\n",
      "grouping by ip-app combination...\n",
      "grouping by ip-app-os combination...\n",
      "grouping by : ip_day_chl_var_hour\n",
      "grouping by : ip_app_os_var_hour\n",
      "grouping by : ip_app_channel_var_day\n",
      "grouping by : ip_app_chl_mean_hour\n",
      "merging...\n",
      "vars and data type: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 203694359 entries, 0 to 203694358\n",
      "Data columns (total 17 columns):\n",
      "app                         uint16\n",
      "channel                     uint16\n",
      "click_id                    float64\n",
      "click_time                  object\n",
      "device                      uint16\n",
      "ip                          uint32\n",
      "is_attributed               float64\n",
      "os                          uint16\n",
      "hour                        uint8\n",
      "day                         uint8\n",
      "ip_tcount                   int64\n",
      "ip_app_count                int64\n",
      "ip_app_os_count             int64\n",
      "ip_tchan_count              float64\n",
      "ip_app_os_var               float64\n",
      "ip_app_channel_var_day      float64\n",
      "ip_app_channel_mean_hour    float64\n",
      "dtypes: float64(6), int64(3), object(1), uint16(4), uint32(1), uint8(2)\n",
      "memory usage: 19.3+ GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "del test_df\n",
    "gc.collect()\n",
    "\n",
    "print('Extracting new features...')\n",
    "train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by ip-day-hour combination...')\n",
    "gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_tcount'})\n",
    "train_df = train_df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by ip-app combination...')\n",
    "gp = train_df[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','app'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print('grouping by ip-app-os combination...')\n",
    "gp = train_df[['ip','app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_os_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Adding features with var and mean hour (inspired from nuhsikander's script)\n",
    "print('grouping by : ip_day_chl_var_hour')\n",
    "gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','channel'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_tchan_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','day','channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_os_var_hour')\n",
    "gp = train_df[['ip','app', 'os', 'hour']].groupby(by=['ip', 'app', 'os'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_app_os_var'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_channel_var_day')\n",
    "gp = train_df[['ip','app', 'channel', 'day']].groupby(by=['ip', 'app', 'channel'])[['day']].var().reset_index().rename(index=str, columns={'day': 'ip_app_channel_var_day'})\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by : ip_app_chl_mean_hour')\n",
    "gp = train_df[['ip','app', 'channel','hour']].groupby(by=['ip', 'app', 'channel'])[['hour']].mean().reset_index().rename(index=str, columns={'hour': 'ip_app_channel_mean_hour'})\n",
    "print(\"merging...\")\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print(\"vars and data type: \")\n",
    "train_df.info()\n",
    "train_df['ip_tcount'] = train_df['ip_tcount'].astype('uint16')\n",
    "train_df['ip_app_count'] = train_df['ip_app_count'].astype('uint16')\n",
    "train_df['ip_app_os_count'] = train_df['ip_app_os_count'].astype('uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle(\"train_and_test_with_features.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int((len(train_df) - len_train) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  175508656\n",
      "valid size:  9395234\n",
      "test size :  18790469\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "test_df = train_df[len_train:]\n",
    "val_df = train_df[(len_train-val_size):len_train]\n",
    "train_df = train_df[:(len_train-val_size)]\n",
    "\n",
    "print(\"train size: \", len(train_df))\n",
    "print(\"valid size: \", len(val_df))\n",
    "print(\"test size : \", len(test_df))\n",
    "\n",
    "target = 'is_attributed'\n",
    "predictors = ['app','device','os', 'channel', 'hour', 'day', \n",
    "              'ip_tcount', 'ip_tchan_count', 'ip_app_count',\n",
    "              'ip_app_os_count', 'ip_app_os_var',\n",
    "              'ip_app_channel_var_day','ip_app_channel_mean_hour']\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"Training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.15,\n",
    "    #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "    'num_leaves': 7,  # 2^max_depth - 1\n",
    "    'max_depth': 3,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'scale_pos_weight':99 # because training data is extremely unbalanced \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_id</th>\n",
       "      <th>click_time</th>\n",
       "      <th>device</th>\n",
       "      <th>ip</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>os</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>ip_tcount</th>\n",
       "      <th>ip_app_count</th>\n",
       "      <th>ip_app_os_count</th>\n",
       "      <th>ip_tchan_count</th>\n",
       "      <th>ip_app_os_var</th>\n",
       "      <th>ip_app_channel_var_day</th>\n",
       "      <th>ip_app_channel_mean_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-06 14:32:21</td>\n",
       "      <td>1</td>\n",
       "      <td>83230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5759</td>\n",
       "      <td>1431</td>\n",
       "      <td>7.893333</td>\n",
       "      <td>36.597949</td>\n",
       "      <td>1.389350</td>\n",
       "      <td>8.521277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-06 14:33:34</td>\n",
       "      <td>1</td>\n",
       "      <td>17357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5245</td>\n",
       "      <td>1451</td>\n",
       "      <td>9.618462</td>\n",
       "      <td>25.959046</td>\n",
       "      <td>1.103880</td>\n",
       "      <td>8.236264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-06 14:34:12</td>\n",
       "      <td>1</td>\n",
       "      <td>35810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2156</td>\n",
       "      <td>462</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>33.348475</td>\n",
       "      <td>1.012026</td>\n",
       "      <td>8.872340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-06 14:34:52</td>\n",
       "      <td>1</td>\n",
       "      <td>45745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10547</td>\n",
       "      <td>2186</td>\n",
       "      <td>5.947712</td>\n",
       "      <td>36.870252</td>\n",
       "      <td>1.070305</td>\n",
       "      <td>10.361702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-06 14:35:08</td>\n",
       "      <td>1</td>\n",
       "      <td>161007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>232</td>\n",
       "      <td>80</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>37.221361</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>11.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app  channel  click_id           click_time  device      ip  is_attributed  \\\n",
       "0    3      379       NaN  2017-11-06 14:32:21       1   83230            0.0   \n",
       "1    3      379       NaN  2017-11-06 14:33:34       1   17357            0.0   \n",
       "2    3      379       NaN  2017-11-06 14:34:12       1   35810            0.0   \n",
       "3   14      478       NaN  2017-11-06 14:34:52       1   45745            0.0   \n",
       "4    3      379       NaN  2017-11-06 14:35:08       1  161007            0.0   \n",
       "\n",
       "   os  hour  day  ip_tcount  ip_app_count  ip_app_os_count  ip_tchan_count  \\\n",
       "0  13    14    6          1          5759             1431        7.893333   \n",
       "1  19    14    6          1          5245             1451        9.618462   \n",
       "2  13    14    6          1          2156              462       15.600000   \n",
       "3  13    14    6          1         10547             2186        5.947712   \n",
       "4  13    14    6          1           232               80       10.800000   \n",
       "\n",
       "   ip_app_os_var  ip_app_channel_var_day  ip_app_channel_mean_hour  \n",
       "0      36.597949                1.389350                  8.521277  \n",
       "1      25.959046                1.103880                  8.236264  \n",
       "2      33.348475                1.012026                  8.872340  \n",
       "3      36.870252                1.070305                 10.361702  \n",
       "4      37.221361                0.619048                 11.428571  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "    'num_leaves': 7,  # 2^max_depth - 1\n",
    "    'max_depth': -1,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'scale_pos_weight':99 # because training data is extremely unbalanced \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing validation datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devthebear_gmail_com/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1040: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/devthebear_gmail_com/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:685: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\ttrain's auc: 0.890017\tvalid's auc: 0.906044\n",
      "[20]\ttrain's auc: 0.903361\tvalid's auc: 0.921425\n",
      "[30]\ttrain's auc: 0.925301\tvalid's auc: 0.935766\n",
      "[40]\ttrain's auc: 0.90578\tvalid's auc: 0.925875\n",
      "[50]\ttrain's auc: 0.912121\tvalid's auc: 0.926025\n",
      "[60]\ttrain's auc: 0.917985\tvalid's auc: 0.924834\n",
      "[70]\ttrain's auc: 0.790693\tvalid's auc: 0.785322\n",
      "[80]\ttrain's auc: 0.886633\tvalid's auc: 0.905304\n",
      "[90]\ttrain's auc: 0.90472\tvalid's auc: 0.911368\n",
      "[100]\ttrain's auc: 0.894504\tvalid's auc: 0.902592\n",
      "[110]\ttrain's auc: 0.893112\tvalid's auc: 0.902996\n",
      "[120]\ttrain's auc: 0.878608\tvalid's auc: 0.887164\n",
      "[130]\ttrain's auc: 0.893404\tvalid's auc: 0.903854\n",
      "[140]\ttrain's auc: 0.885748\tvalid's auc: 0.892304\n",
      "[150]\ttrain's auc: 0.880795\tvalid's auc: 0.885618\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's auc: 0.945608\tvalid's auc: 0.95557\n",
      "\n",
      "Model Report\n",
      "n_estimators :  1\n",
      "auc: 0.9555695733646141\n"
     ]
    }
   ],
   "source": [
    "bst = lgb_modelfit_nocv(params, \n",
    "                        train_df, \n",
    "                        val_df, \n",
    "                        predictors, \n",
    "                        target, \n",
    "                        objective='binary', \n",
    "                        metrics='auc',\n",
    "                        early_stopping_rounds=150, \n",
    "                        verbose_eval=True, \n",
    "                        num_boost_round=500, \n",
    "                        learning_rates=lambda iter: 0.10 * (0.99 ** iter),\n",
    "                        categorical_features=categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01,\n",
       " 'num_leaves': 7,\n",
       " 'max_depth': 3,\n",
       " 'min_child_samples': 100,\n",
       " 'max_bin': 100,\n",
       " 'subsample': 0.7,\n",
       " 'subsample_freq': 1,\n",
       " 'colsample_bytree': 0.9,\n",
       " 'min_child_weight': 0,\n",
       " 'scale_pos_weight': 99}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_no_params(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc', categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.1,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "#         'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': 4,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.8,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "        \"colsample_bylevel\" : 0.7,\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "#         'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "#         'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 4,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 12,\n",
    "        'verbose': 0,\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results,\n",
    "                     verbose_eval=10)\n",
    "\n",
    "    n_estimators = bst1.best_iteration\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][n_estimators-1])\n",
    "\n",
    "    return bst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing validation datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devthebear_gmail_com/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1040: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/devthebear_gmail_com/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:685: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\ttrain's auc: 0.921082\tvalid's auc: 0.938199\n",
      "[20]\ttrain's auc: 0.897692\tvalid's auc: 0.92921\n",
      "[30]\ttrain's auc: 0.924615\tvalid's auc: 0.947206\n",
      "[40]\ttrain's auc: 0.910762\tvalid's auc: 0.938206\n",
      "[50]\ttrain's auc: 0.933031\tvalid's auc: 0.952039\n",
      "[60]\ttrain's auc: 0.828081\tvalid's auc: 0.93529\n",
      "[70]\ttrain's auc: 0.856619\tvalid's auc: 0.941277\n",
      "[80]\ttrain's auc: 0.913717\tvalid's auc: 0.952951\n",
      "[90]\ttrain's auc: 0.812587\tvalid's auc: 0.936791\n",
      "[100]\ttrain's auc: 0.919568\tvalid's auc: 0.95255\n",
      "\n",
      "Model Report\n",
      "n_estimators :  0\n",
      "auc: 0.9525502109993329\n"
     ]
    }
   ],
   "source": [
    "bst2 = lgb_no_params({}, \n",
    "                        train_df, \n",
    "                        val_df, \n",
    "                        predictors, \n",
    "                        target, \n",
    "                        objective='binary', \n",
    "                        metrics='auc',\n",
    "                    categorical_features=categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing...\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "sub['is_attributed'] = bst2.predict(test_df[predictors])\n",
    "print(\"writing...\")\n",
    "sub.to_csv('first_subm.csv',index=False)\n",
    "print(\"done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing...\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "sub['is_attributed'] = bst.predict(test_df[predictors])\n",
    "print(\"writing...\")\n",
    "sub.to_csv('second_subm.csv',index=False)\n",
    "print(\"done...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
